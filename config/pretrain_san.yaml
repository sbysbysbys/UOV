dataset : "nuscenes"
working_dir : "output/slidr/nuscenes/"

# SAN
text_number : 175
lidarseg_ov_dir : "/root/wangfeiyue3new/sby/.data/SAN/lidarseg_ov"
text_embedding_path : "/root/wangfeiyue3new/sby/.data/SAN/superpixels/4text_embedding.pt"
feature_dim : 768
sp_dir : "/root/wangfeiyue3new/sby/.data/SAN/superpixels"
superpixels_type : "SAN" 
model_n_out : 768           
alpha_i : 0.5
alpha_t : 0.5   

num_epochs : 40
# number of GPUs and CPU threads to use
num_gpus : 4

# if set to True, use cylindrical coordinates, otherwise use cartesian
cylindrical_coordinates : True
# size of the voxel in each dimension for cartesian coordinates,
# and in rho and z for cylindrical (angular is always 1Â°)
voxel_size : 0.1
batch_size : 4
# learning rate
lr : 0.5
sgd_momentum : 0.9
sgd_dampening : 0.1
weight_decay : 0.0001
# used in superpixel loss only, drop points and pixels from the computation of the loss
dropout : 0.

num_threads : 4
kernel_size : 3

bn_momentum : 0.05
crop_size : [224, 416]
crop_ratio : [1.5555555555555556, 1.8888888888888888]
# point cloud backbone to use among "minkunet" and "voxelnet"
model_points : "minkunet"
# which image pretraining to chose among:
# 'imagenet','obow', 'pixpro', 'moco_v1', 'moco_v2', 'swav',
# 'deepcluster_v2', 'dino', 'moco_coco'
image_weights : "moco_v2"
# which image encoder to use (only imagenet is available with resnet18)
images_encoder : "resnet50"
# which image decoder to use
# 'bilinear', 'unet', 'fpn', 'semseg', 'nnfe', 'dilation', 'ppkt'
decoder : "dilation"
# temperature parameter in the InfoNCE loss
NCE_temperature : 0.07
# number of positive matches in the InfoNCE loss
num_matches : 4096
# whether to use the true validation set or the custom parametrization set
training : "parametrize"
# transformations to apply to the clouds
transforms_clouds : ["Rotation", "FlipAxis"]
# transformations to apply to both the clouds and the images among:
# 'FlipHorizontal', 'DropCuboids', 'ResizedCrop'
transforms_mixed : ["DropCuboids", "ResizedCrop", "FlipHorizontal"]
# which losses to use (note that multiple losses will be summed)
# loss_per_scene, loss_superpixels, loss_superpixels_reduce,
# loss_superpixels_reduce_all, loss_superpixels_transforms
losses : ["loss_superpixels_average"]
# only keep 1 in dataset_skip_step training examples (here use 100% of the data)
dataset_skip_step : 1
# path to weights to continue a previous training
resume_path : Null

# WARNING: DO NOT CHANGE THE FOLLOWING PARAMETERS
# ===============================================
normalize_features : True
superpixel_size : 150
